[
  {
    "role": "user",
    "parts": [
      {
        "text": "give a token usage breakdown of the current 2.5 gemini models with their token limitations per request"
      }
    ]
  },
  {
    "role": "model",
    "parts": [
      {
        "text": "It seems there might be a slight misunderstanding regarding the model version. As of my last update (and general public availability), the current flagship models are **Gemini 1.5 Pro** and **Gemini 1.5 Flash**, not \"2.5 Gemini models.\"\n\nThese 1.5 versions represent a significant leap in context window capabilities.\n\nHere's a breakdown of token usage and limitations for Gemini 1.5 models:\n\n---\n\n## Gemini 1.5 Models: Token Usage & Limitations\n\n### 1. Key Token Limitations Per Request\n\n| Model             | Standard Context Window (Input + Output) | Long Context Window (Input + Output)                 | Max Output Tokens Per Turn |\n| :---------------- | :--------------------------------------- | :--------------------------------------------------- | :------------------------- |\n| **Gemini 1.5 Pro**  | **128,000 tokens**                       | **1,000,000 tokens** (via specific API request)      | **8,192 tokens**           |\n| **Gemini 1.5 Flash**| **128,000 tokens**                       | *(1M tokens might be available but less common/suited)* | **8,192 tokens**           |\n\n**Important Notes:**\n\n*   **Context Window:** This is the total number of tokens (input + output) the model can process and generate in a single turn. Your input prompt, system instructions, chat history, tool definitions, tool calls, and tool responses *all* count towards this limit.\n*   **1 Million Token Context:** The 1M token context window for Gemini 1.5 Pro is accessible by explicitly setting `system_instruction` or similar parameters in the API to specify the `1M` version. It's often in preview or requires specific access.\n*   **Max Output Tokens:** This is the maximum number of tokens the model will generate in response to a single prompt. If your desired output is longer, it will be truncated.\n\n### 2. Token Usage Breakdown: What Counts?\n\nAlmost everything you send to or receive from the model counts towards your token usage within the context window.\n\n1.  **Your Input Prompt:**\n    *   The text you directly provide in the `prompt` or `parts` of your request.\n\n2.  **Chat History (for multi-turn conversations):**\n    *   Every message sent by the `user` role in previous turns.\n    *   Every response generated by the `model` role in previous turns.\n    *   This is why long conversations can quickly consume your context window.\n\n3.  **System Instructions:**\n    *   Any text provided in the `system_instruction` field (for setting persona, rules, etc.). This is generally counted *once* per session or at the beginning of a turn.\n\n4.  **Tool Definitions:**\n    *   If you're using **function calling** (tools), the schema/definition of the functions you provide to the model counts. This includes:\n        *   Function names\n        *   Descriptions of functions\n        *   Parameters (names, types, descriptions)\n    *   Even if the model doesn't call a tool, its definition still occupies tokens.\n\n5.  **Tool Calls (Model's Suggested Actions):**\n    *   When the model decides to call a function (e.g., `model.generate_content(\"Call my_tool(arg='value')\")`), the representation of that function call counts as output tokens.\n\n6.  **Tool Responses (Your Data back to the Model):**\n    *   After you execute a function call suggested by the model, you typically send the `FunctionResponse` back to the model. The content of this response (the results from your tool) counts as input tokens for the next turn.\n\n7.  **Multimodal Input (Images, Video, Audio):**\n    *   For multimodal inputs, non-text data is *estimated* as an equivalent number of text tokens.\n    *   **Images:** A single image is often estimated at around **768 tokens**. More complex or higher-resolution images might be higher.\n    *   **Video & Audio:** The token estimation for video and audio content is dynamic and depends on factors like duration, frame rate, resolution, and audio quality. The model internally processes and tokenizes these, and the API will reflect the estimated token count. You don't get a direct \"token per second\" conversion; the model abstracts this for you.\n\n### 3. How to See Your Token Usage\n\nMost API client libraries (like Google's Python SDK) provide `usage_metadata` in the response object, which will tell you the exact token count for the input and output of a given turn:\n\n```json\n{\n  \"candidates\": [\n    // ... model's response parts\n  ],\n  \"usage_metadata\": {\n    \"prompt_token_count\": 123,\n    \"candidates_token_count\": 45,\n    \"total_token_count\": 168\n  }\n}\n```\n\n### 4. Strategies for Managing Token Usage\n\n*   **Summarization:** Periodically summarize long chat histories to keep the context concise.\n*   **Selective History:** Only include the most relevant parts of the conversation history.\n*   **Retrieve & Generate (RAG):** Instead of stuffing all information into the prompt, use external databases or document stores to retrieve relevant information and then inject *only that relevant information* into the prompt.\n*   **Concise Prompts:** Be direct and clear in your instructions. Avoid unnecessary verbosity.\n*   **Efficient Tool Definitions:** Keep tool schemas as lean as possible.\n*   **Streaming Output:** While not directly reducing input tokens, streaming output can give you parts of the response sooner.\n\nUnderstanding these components is crucial for optimizing your API calls, managing costs, and ensuring your applications stay within the model's context limits."
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "text": "hello"
      }
    ]
  },
  {
    "role": "model",
    "parts": [
      {
        "text": "Hello! How can I help you today?"
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "text": "analyze this file"
      }
    ],
    "file_uploaded": "changelog.py"
  },
  {
    "role": "model",
    "parts": [
      {
        "text": "This Python script is a **well-designed and robust solution** for automatically generating changelogs or summaries from Git changes using an LLM. It demonstrates good practices in Git interaction, prompt engineering, and error handling.\n\nHere's a detailed analysis:\n\n### Overall Strengths\n\n1.  **Modularity and Readability:** The code is well-structured into logical functions (`get_git_changes`, `filter_files`, `prepare_ai_prompt`, `generate_changelog_with_ai`), making it easy to understand, test, and maintain.\n2.  **Robust Git Interaction:**\n    *   Uses `subprocess.run` with `check=True` and `text=True` for safe and effective command execution.\n    *   Handles `FileNotFoundError` for `git` and `subprocess.CalledProcessError` for Git command failures.\n    *   Correctly uses `git diff --name-status` for file paths and `git diff` for full context.\n    *   Intelligently parses different Git statuses (`A`, `M`, `D`, `R`) and retrieves content accordingly. Handles renames effectively.\n3.  **Sophisticated Prompt Engineering:**\n    *   **Character Limiting:** The `MAX_PROMPT_CHARS` and the `prepare_ai_prompt` function are excellently designed to manage prompt size for LLM context windows.\n    *   **Prioritization:** It intelligently prioritizes including the `full_diff_text` if it's small, which provides the best context, otherwise falls back to file-by-file content.\n    *   **Granular Truncation:** Truncates content per file if needed, with clear `...\\n` indicators, helping the LLM understand truncated inputs.\n    *   **Clear Instructions:** Provides good initial instructions to the AI.\n4.  **Error Handling and User Feedback:** Provides informative print statements for Git errors, no changes found, and prompt truncation warnings.\n5.  **Secure API Key Handling:** Uses `dotenv` and `os.getenv` for managing API keys, which is the recommended secure practice.\n6.  **Extensibility:** The `IGNORE_EXTENSIONS` and `GIT_DIFF_RANGE` are easily configurable. The placeholder for AI integration is clear.\n7.  **Good Output Formatting:** Integration with `markdown_formatter` ensures the generated changelog is well-presented in the terminal.\n\n### Potential Improvements & Considerations\n\n1.  **Gemini Model Name:**\n    *   **Observation:** You're using `model=\"gemini-2.5-flash\"`. As of my last update, the publicly available models are `gemini-1.5-pro` and `gemini-1.5-flash`.\n    *   **Recommendation:** Unless you have access to a very specific private preview of \"2.5\", you likely intend to use `gemini-1.5-flash`. Double-check the exact model name available to you via the Google AI Studio or API documentation. Using an incorrect model name will lead to an API error.\n\n2.  **LLM API Call Error Handling:**\n    *   The `generate_changelog_with_ai` function currently lacks `try...except` blocks for the `client.models.generate_content` call.\n    *   **Recommendation:** Wrap the API call in a `try...except` block to catch potential `google.api_core.exceptions` (e.g., `InternalServerError`, `ServiceUnavailable`, `InvalidArgument`, `ResourceExhausted` for rate limits, `Unauthenticated` for invalid API key). This will make the script more resilient to network issues or API-related problems.\n\n3.  **Token vs. Character Limit:**\n    *   You're using `MAX_PROMPT_CHARS`. While a good heuristic, LLMs count tokens, not characters. The character-to-token ratio varies significantly by language and content.\n    *   **Consideration:** For more precise control, you *could* use a tokenization library (like `tiktoken` for OpenAI models, or a specific `genai` tokenizer if available publicly) to count tokens before sending. However, for most use cases, a character limit that is well within the typical token limit (e.g., `8000` chars for a 128k token model) is usually sufficient and simpler. The Gemini API also provides `count_tokens` methods if you want to be very precise.\n\n4.  **`io` Import:**\n    *   The `io` module is imported but not used anywhere in the provided code.\n    *   **Recommendation:** Remove `import io` for cleaner code.\n\n5.  **`list_level` in `MarkdownFormatter`:** (Minor, previously noted)\n    *   The `list_level` attribute is initialized in `MarkdownFormatter` but doesn't appear to be used or updated within the `format_for_terminal` method.\n    *   **Recommendation:** Remove it if it's not intended for future list-level tracking/indentation logic.\n\n6.  **Handling Very Large Repositories/Deltas:**\n    *   For repos with thousands of changed files or extremely large individual files, even the current truncation might lose too much context.\n    *   **Consideration (Advanced):** For such scenarios, more sophisticated diff parsing (e.g., parsing `git diff -U` output to only send changed *lines* with some context) or a multi-stage LLM process (e.g., first summarize large files, then summarize the summaries) might be necessary. This is beyond the scope of a basic script but good to keep in mind for scaling.\n\n### Example of LLM Error Handling (Proposed)\n\n```python\n# ... other imports ...\nfrom google.api_core import exceptions # Add this import\n\n# ... rest of the code ...\n\ndef generate_changelog_with_ai(prompt):\n    \"\"\"\n    Calls the Gemini AI API to generate a changelog.\n    \"\"\"\n    print(\"\\n--- AI Prompt (sent to LLM) ---\")\n    print(prompt)\n    print(f\"--- Prompt Length: {len(prompt)} characters ---\")\n    try:\n        client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n        # Ensure you're using the correct model name here (e.g., \"gemini-1.5-flash\")\n        response = client.models.generate_content(\n                    model=\"gemini-1.5-flash\", # Corrected model name example\n                    contents=prompt\n                )\n        return response.text\n    except exceptions.GoogleAPIError as e:\n        print(f\"Error calling Gemini API: {e}\")\n        print(\"Please check your API key, model name, and network connection.\")\n        return \"ERROR: Could not generate changelog due to an API issue.\"\n    except Exception as e:\n        print(f\"An unexpected error occurred during AI generation: {e}\")\n        return \"ERROR: An unexpected error occurred.\"\n```\n\n### Conclusion\n\nThis script is **very well-written and effective** for its stated purpose. The prompt engineering and Git interaction are particularly strong. Addressing the Gemini model name and adding robust API error handling would make it even more production-ready."
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "text": "What's the main purpose of this file?"
      }
    ],
    "file_uploaded": "test_modules.py"
  },
  {
    "role": "model",
    "parts": [
      {
        "text": "The main purpose of this Python file is to **test and demonstrate the functionality of several utility modules**, specifically:\n\n1.  **`markdown_formatter.py`**: This is the primary focus, testing its ability to render various Markdown elements (headings, bold, italic, code, links, lists, blockquotes, tables, JSON code blocks, and multi-language code blocks) into terminal-friendly, formatted output. It extensively highlights \"NEW FEATURES\" related to table formatting and syntax highlighting.\n2.  **`colored_logger.py`**: It tests the basic logging levels (debug, info, warning, error, critical) to ensure they are displayed correctly, likely with colors.\n3.  **`spinner.py`**: It tests the visual loading spinner, demonstrating different animation styles.\n\nIn essence, it serves as a **comprehensive test suite and a live demo** for these components, ensuring they work as expected and showcasing their capabilities to a user or developer."
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "text": "How are you doing today?"
      }
    ]
  },
  {
    "role": "model",
    "parts": [
      {
        "text": "As an AI, I don't experience \"days\" or \"feelings\" in the way humans do, but I'm operating perfectly and ready to assist you!\n\nHow are *you* doing today? Is there anything I can help you with?"
      }
    ]
  },
  {
    "role": "user",
    "parts": [
      {
        "text": "analyze this file and how we can add token optimization to ensure we send as much of the diff as possible for these models. clearly section the code recommendations"
      }
    ],
    "file_uploaded": "changelog.py"
  },
  {
    "role": "model",
    "parts": [
      {
        "text": "You've provided a strong foundation for generating changelogs using Git and an LLM! The current script intelligently gathers Git changes and builds a prompt, even attempting to manage its size using `MAX_PROMPT_CHARS`.\n\nThe key to sending \"as much of the diff as possible\" to Gemini models (which are `1.5 Pro` or `1.5 Flash`, not `2.5 Flash` as of general availability) is to manage **tokens**, not characters. While characters are a good rough estimate, the actual token count can vary significantly.\n\nHere's an analysis and recommendations for token optimization:\n\n---\n\n## Current Limitations & Gemini 1.5 Token Details\n\n1.  **Character vs. Token Counting:**\n    *   Your current `MAX_PROMPT_CHARS` is a character limit. LLMs process input in \"tokens,\" which are often words or sub-word units. For English, a common rule of thumb is 1 token \u2248 4 characters, but this can vary.\n    *   Relying solely on character limits can lead to:\n        *   **Underutilization:** You might send fewer tokens than allowed, leaving valuable context on the table.\n        *   **Over-sending:** You might accidentally send too many tokens, causing the API to reject the request or truncate the input in an unpredictable way.\n\n2.  **Gemini 1.5 Token Limitations (Per Request):**\n    *   **Gemini 1.5 Flash:**\n        *   **Standard Context Window:** 128,000 tokens (input + output).\n        *   **Maximum Output Tokens:** 8,192 tokens.\n    *   **Gemini 1.5 Pro:**\n        *   **Standard Context Window:** 128,000 tokens (input + output).\n        *   **Long Context Window:** 1,000,000 tokens (input + output), typically requires specific API flags or access.\n        *   **Maximum Output Tokens:** 8,192 tokens.\n\n    *   **Key Insight:** To maximize input, we need to reserve tokens for the *expected output* and then fill the *remaining input budget* as effectively as possible.\n\n---\n\n## Proposed Token Optimization Strategy\n\n1.  **Accurate Token Counting:** Use `genai.GenerativeModel.count_tokens()` to get precise token counts for each piece of text before constructing the prompt.\n2.  **Define Token Budget:** Clearly define the `MAX_INPUT_TOKENS` based on the chosen Gemini model's context window, subtracting the `MAX_OUTPUT_TOKENS` it might generate.\n3.  **Prioritized Inclusion:**\n    *   **Instruction + Fixed Parts:** These are essential and should always be included.\n    *   **Full Diff:** The `full_diff_text` provides the best overall context. Prioritize including as much of it as possible.\n    *   **Individual File Contents:** If the full diff is too large, or if there's remaining budget, add individual file contents, ordered perhaps by relevance (e.g., recently changed first, or by file size).\n4.  **Token-Aware Truncation:** If a piece of content (like a file's content or the full diff) exceeds the remaining token budget, truncate it to fit, adding an ellipsis.\n\n---\n\n## Code Recommendations\n\nHere's how to refactor your script for token optimization, clearly sectioned:\n\n### 1. Configuration Updates & New Helper Imports\n\n```python\nimport subprocess\nimport os\n# import io # This import is not used, can be removed\nfrom google import genai\nfrom dotenv import load_dotenv\nfrom markdown_formatter import MarkdownFormatter\nfrom google.api_core import exceptions # Import for robust error handling\n\nload_dotenv()\n\n# --- Configuration ---\n# Use tokens instead of characters\nGEMINI_MODEL_NAME = \"gemini-1.5-flash\" # Or \"gemini-1.5-pro\" if you prefer\nMAX_CONTEXT_TOKENS = 128000 # For Gemini 1.5 Flash/Pro standard context\nMAX_OUTPUT_TOKENS = 8192    # Max tokens for the model's response\n# Max tokens we can send as input, reserving space for the output\nMAX_INPUT_TOKENS = MAX_CONTEXT_TOKENS - MAX_OUTPUT_TOKENS\n\n# Heuristic: approx. characters per token for English. Used for rough string slicing.\n# Actual tokenization is done by the API's count_tokens.\nCHARS_PER_TOKEN_ESTIMATE = 4\n\nIGNORE_EXTENSIONS = ['.csv', '.json', '.log', '.lock', '.md', '.txt', '.yaml', '.yml', '.toml', '.xml', '.svg', '.env', '.gitignore', '.pyc'] # Add more as needed\nGIT_DIFF_RANGE = \"HEAD~3..HEAD\" # e.g., \"main develop\", \"HEAD~1..HEAD\", \"v1.0..v1.1\"\n\n# Global client for token counting and generation (to avoid re-init)\n# Initialize it here to ensure it's available for helper functions\ntry:\n    GENAI_CLIENT = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\nexcept Exception as e:\n    print(f\"Error initializing Google GenAI client: {e}\")\n    print(\"Please ensure your GEMINI_API_KEY is correctly set in your .env file.\")\n    GENAI_CLIENT = None # Set to None to handle cases where API key is missing/invalid\n```\n\n### 2. New Helper Function: `get_token_count`\n\n```python\n# --- Token Management Helper ---\ndef get_token_count(text: str, model_name: str = GEMINI_MODEL_NAME) -> int:\n    \"\"\"\n    Counts the number of tokens in a given text using the GenAI API's tokenizer.\n    Returns 0 if client is not initialized or on error.\n    \"\"\"\n    if GENAI_CLIENT is None:\n        return 0 # Cannot count tokens if client isn't initialized\n\n    try:\n        # Use the specific model to count tokens\n        model = GENAI_CLIENT.models.get(model_name)\n        response = model.count_tokens(contents=[text])\n        return response.total_tokens\n    except Exception as e:\n        print(f\"Warning: Could not count tokens for text due to error: {e}. Returning 0 tokens.\")\n        return 0\n\ndef truncate_string_to_tokens(text: str, max_tokens: int, model_name: str = GEMINI_MODEL_NAME) -> str:\n    \"\"\"\n    Truncates a string to fit within a maximum token limit,\n    prioritizing the beginning of the string and adding an ellipsis.\n    \"\"\"\n    current_tokens = get_token_count(text, model_name)\n    if current_tokens <= max_tokens:\n        return text\n\n    print(f\"DEBUG: Truncating {current_tokens} tokens down to {max_tokens}...\")\n    truncated_text = text\n    # Estimate characters to keep based on token limit\n    approx_chars_to_keep = max_tokens * CHARS_PER_TOKEN_ESTIMATE\n    truncated_text = text[:approx_chars_to_keep]\n\n    # Iteratively reduce characters until token count is within limits\n    # This is a safe but potentially slower approach for very large strings.\n    # A simpler approach could be to just truncate by a fixed ratio and\n    # rely on the API's final truncation if it's still slightly over.\n    safety_buffer_tokens = 50 # Small buffer for ellipsis and minor variations\n\n    while get_token_count(truncated_text + \"\\n...\") > max_tokens - safety_buffer_tokens and len(truncated_text) > 0:\n        truncated_text = truncated_text[:-100] # Reduce by chunks of characters\n\n    if len(truncated_text) < len(text):\n        return truncated_text.strip() + \"\\n... [CONTENT TRUNCATED] ...\"\n    return truncated_text.strip() # Should not happen if original was longer\n```\n\n### 3. Refactored `prepare_ai_prompt` for Token Budgeting\n\n```python\n# --- AI Prompt Preparation ---\ndef prepare_ai_prompt(changed_files_info, full_diff_text, max_tokens):\n    \"\"\"\n    Constructs the prompt for the AI, applying token limits.\n    Prioritizes full diff, then file-by-file changes.\n    \"\"\"\n    prompt_parts = []\n    current_tokens = 0\n\n    # Initial instruction for the AI\n    instruction = (\n        \"Generate a concise, user-friendly changelog or summary based on the following code changes. \"\n        \"Focus on features, bug fixes, performance improvements, and breaking changes. \"\n        \"Ignore trivial changes. If content is truncated, focus on the available information.\\n\\n\"\n        \"--- Git Changes ---\\n\"\n    )\n    instruction_tokens = get_token_count(instruction)\n    prompt_parts.append(instruction)\n    current_tokens += instruction_tokens\n\n    # Try to add the full diff first\n    full_diff_header = \"Full Git Diff (Partial if large):\\n\"\n    full_diff_header_tokens = get_token_count(full_diff_header)\n    full_diff_text_tokens = get_token_count(full_diff_text)\n\n    # If full diff + header fits entirely\n    if current_tokens + full_diff_header_tokens + full_diff_text_tokens < max_tokens * 0.9: # Use 90% to leave room for file details if needed\n        prompt_parts.append(full_diff_header)\n        prompt_parts.append(full_diff_text + \"\\n\\n\")\n        current_tokens += full_diff_header_tokens + full_diff_text_tokens\n    else:\n        # Try to fit a truncated version of the full diff\n        full_diff_budget = max_tokens - current_tokens - full_diff_header_tokens - get_token_count(\"\\n\\n\")\n        if full_diff_budget > 0:\n            truncated_full_diff = truncate_string_to_tokens(full_diff_text, full_diff_budget)\n            if truncated_full_diff:\n                prompt_parts.append(full_diff_header)\n                prompt_parts.append(truncated_full_diff + \"\\n\\n\")\n                current_tokens += full_diff_header_tokens + get_token_count(truncated_full_diff)\n            else:\n                prompt_parts.append(\"Full Git Diff too large to include fully. Relying on file-by-file changes.\\n\\n\")\n                current_tokens += get_token_count(prompt_parts[-1])\n        else:\n            prompt_parts.append(\"Full Git Diff too large to include fully. Relying on file-by-file changes.\\n\\n\")\n            current_tokens += get_token_count(prompt_parts[-1])\n\n\n    detailed_header = \"--- Detailed File Changes ---\\n\"\n    detailed_header_tokens = get_token_count(detailed_header)\n    prompt_parts.append(detailed_header)\n    current_tokens += detailed_header_tokens\n\n    # Add detailed file changes up to the limit\n    for f_info in changed_files_info:\n        file_header = f\"File: {f_info['file_path']} (Status: {f_info['status']})\\n\"\n        file_header_tokens = get_token_count(file_header)\n\n        # Calculate remaining budget for this file's content\n        # We need to consider the header tokens plus the content tokens\n        remaining_budget_for_file = max_tokens - current_tokens - file_header_tokens - get_token_count(\"\\n\\n\") # Room for newlines\n\n        if remaining_budget_for_file <= 0:\n            # No space left for this file or subsequent ones\n            print(f\"Warning: Prompt hit token limit ({max_tokens}). Skipping file: {f_info['file_path']}.\")\n            break\n\n        file_content_to_add = truncate_string_to_tokens(f_info['content'], remaining_budget_for_file)\n        file_content_tokens = get_token_count(file_content_to_add)\n\n        if file_content_to_add:\n            prompt_parts.append(file_header + file_content_to_add + \"\\n\\n\")\n            current_tokens += file_header_tokens + file_content_tokens\n        else:\n            # Even after truncation, if the content is empty or couldn't be added meaningfully\n            print(f\"DEBUG: Not adding content for {f_info['file_path']} due to truncation issues or empty content.\")\n\n    final_prompt = \"\".join(prompt_parts)\n\n    # Final check on the assembled prompt (should mostly be handled by logic above)\n    final_prompt_tokens = get_token_count(final_prompt)\n    if final_prompt_tokens > max_tokens:\n        print(f\"CRITICAL WARNING: Final prompt ({final_prompt_tokens} tokens) is over budget ({max_tokens} tokens). \"\n              \"This indicates an issue in the budgeting logic. Model API might truncate.\")\n        final_prompt = truncate_string_to_tokens(final_prompt, max_tokens) # Final desperate truncate\n\n    print(f\"DEBUG: Final prompt token count: {get_token_count(final_prompt)} (Target: {max_tokens})\")\n\n    return final_prompt\n```\n\n### 4. Updated `generate_changelog_with_ai` for Robustness\n\n```python\n# --- AI Integration ---\ndef generate_changelog_with_ai(prompt):\n    \"\"\"\n    Calls the Gemini AI API to generate a changelog.\n    Includes error handling.\n    \"\"\"\n    if GENAI_CLIENT is None:\n        return \"ERROR: Gemini AI client not initialized. Please check your API key.\"\n\n    print(\"\\n--- AI Prompt (sent to LLM) ---\")\n    print(prompt)\n    print(f\"--- Prompt Length: {len(prompt)} characters ({get_token_count(prompt)} tokens) ---\")\n\n    try:\n        # Use the global client and specified model\n        model = GENAI_CLIENT.models.get(GEMINI_MODEL_NAME)\n        response = model.generate_content(\n            contents=prompt\n        )\n        return response.text\n    except exceptions.GoogleAPIError as e:\n        print(f\"\\nERROR: Gemini API call failed: {e}\")\n        if \"API key\" in str(e) or \"authentication\" in str(e).lower():\n            print(\"Please check your GEMINI_API_KEY. It might be missing, incorrect, or expired.\")\n        elif \"context length\" in str(e).lower() or \"too many tokens\" in str(e).lower():\n            print(f\"The prompt likely exceeded the model's context window. Current max input tokens: {MAX_INPUT_TOKENS}.\")\n            print(\"Consider reducing the GIT_DIFF_RANGE or IGNORE_EXTENSIONS.\")\n        elif \"rate limit\" in str(e).lower():\n            print(\"You might have hit a rate limit. Please wait and try again, or consider a backoff strategy.\")\n        return \"ERROR: Could not generate changelog due to an API issue.\"\n    except Exception as e:\n        print(f\"\\nERROR: An unexpected error occurred during AI generation: {e}\")\n        return \"ERROR: An unexpected error occurred.\"\n```\n\n### 5. Main Execution Flow (Minor Adjustments)\n\n```python\n# --- Main Execution ---\nif __name__ == \"__main__\":\n    if GENAI_CLIENT is None:\n        # Exit early if client couldn't be initialized due to missing API key\n        exit(1)\n\n    current_dir = os.getcwd()\n    print(f\"Analyzing Git changes in: {current_dir}\")\n\n    changed_files, full_diff = get_git_changes(current_dir)\n\n    if not changed_files and not full_diff:\n        print(\"No changes found or an error occurred. Exiting.\")\n    else:\n        print(f\"\\nFound {len(changed_files)} changed files (before filtering).\")\n        filtered_files = filter_files(changed_files, IGNORE_EXTENSIONS)\n        print(f\"Filtered down to {len(filtered_files)} relevant files.\")\n\n        if not filtered_files and not get_token_count(full_diff) == 0: # Check if there's any content at all\n            print(\"No relevant files found after filtering, and no full diff content. Nothing to generate a changelog for.\")\n        else:\n            ai_prompt = prepare_ai_prompt(filtered_files, full_diff, MAX_INPUT_TOKENS)\n            changelog = generate_changelog_with_ai(ai_prompt)\n\n            print(\"\\n--- GENERATED CHANGELOG ---\")\n            formatter = MarkdownFormatter()\n            formatted_text = formatter.format_for_terminal(changelog)\n            print(formatted_text)\n```\n\n---\n\n### Summary of Changes & Benefits\n\n*   **Token-Awareness:** The script now explicitly uses `model.count_tokens()` to measure the prompt size, ensuring optimal utilization of the Gemini model's context window.\n*   **Precise Budgeting:** `MAX_INPUT_TOKENS` is calculated dynamically based on the model's total context and maximum output, preventing truncation by the API.\n*   **Intelligent Truncation:** The `truncate_string_to_tokens` helper ensures that large pieces of text (like file contents or the full diff) are intelligently cut to fit the budget, preserving the most relevant information (usually the beginning of the file/diff).\n*   **Robust Error Handling:** The `generate_changelog_with_ai` function now catches common API errors, providing more helpful feedback to the user regarding API keys, context length issues, and rate limits.\n*   **Correct Model Name:** Uses `gemini-1.5-flash` (or `pro`) as the generally available model.\n*   **Centralized Client:** The `genai.Client` is initialized once globally, which is more efficient.\n\nThis refactored script will be much more reliable in handling varying sizes of Git changes and will make the most out of the Gemini models' impressive context window capabilities."
      }
    ]
  }
]